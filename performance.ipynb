{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# performance analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import label_binarize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history):\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    epochs = range(1, len(acc) + 1)\n",
    "\n",
    "    plt.figure(figsize=(14, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, acc, 'bo-', label='Training accuracy')\n",
    "    plt.plot(epochs, val_acc, 'ro-', label='Validation accuracy')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, loss, 'bo-', label='Training loss')\n",
    "    plt.plot(epochs, val_loss, 'ro-', label='Validation loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cf_matrix(model, test_generator):\n",
    "    test_generator.reset()\n",
    "    Y_pred = model.predict(test_generator)\n",
    "    y_pred = np.argmax(Y_pred, axis=1)\n",
    "    class_names = list(test_generator.class_indices.keys())\n",
    "\n",
    "    print('Confusion Matrix')\n",
    "    conf_metrix = confusion_matrix(test_generator.classes, y_pred)\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(conf_metrix, annot=True, fmt='g', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.xlabel('Predicted labels')\n",
    "    plt.ylabel('True labels')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Misclassifications plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_misclassifications(model, test_generator):\n",
    "    test_generator.reset()\n",
    "    Y_pred = model.predict(test_generator)\n",
    "    y_pred = np.argmax(Y_pred, axis=1)\n",
    "    class_names = list(test_generator.class_indices.keys())\n",
    "    conf_metrix = confusion_matrix(test_generator.classes, y_pred)\n",
    "    \n",
    "    print('Misclassifications')\n",
    "    misclassifications = np.sum(conf_metrix, axis=1) - np.diag(conf_metrix)\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.bar(class_names, misclassifications, color='red')\n",
    "    plt.xlabel('Class Labels')\n",
    "    plt.ylabel('Number of Misclassifications')\n",
    "    plt.title('Misclassifications for Each Class')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reports(model, test_generator):\n",
    "    test_generator.reset()\n",
    "    Y_pred = model.predict(test_generator)\n",
    "    y_pred = np.argmax(Y_pred, axis=1)\n",
    "    class_names = list(test_generator.class_indices.keys())\n",
    "\n",
    "    print('Classification Reports')\n",
    "    print(classification_report(test_generator.classes, y_pred, target_names=class_names))\n",
    "\n",
    "    test_loss, test_acc = model.evaluate(test_generator)\n",
    "    print(\"Test Accuracy:\", test_acc)\n",
    "    print(\"Test Loss:\", test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AUC-ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve_with_auc(model, generator):\n",
    "    class_labels = list(generator.class_indices.keys())\n",
    "    num_classes = len(class_labels)\n",
    "    y_true = generator.classes  \n",
    "    y_pred = model.predict(generator)\n",
    "    y_true_bin = label_binarize(y_true, classes=list(range(num_classes)))\n",
    "\n",
    "    auc_scores = roc_auc_score(y_true_bin, y_pred, multi_class='ovr', average=None)\n",
    "    \n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    for i in range(num_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_true_bin[:, i], y_pred[:, i])\n",
    "    \n",
    "    # Plot ROC curve for each class\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for i in range(num_classes):\n",
    "        plt.plot(fpr[i], tpr[i], lw=2, label=f'{class_labels[i]} (AUC = {auc_scores[i]:.2f})')\n",
    "    \n",
    "    # Plot the diagonal line for a random classifier\n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_measurement(history,model,generator):\n",
    "    plot_training_history(history)\n",
    "    plot_cf_matrix(model,generator)\n",
    "    reports(model,generator)\n",
    "    plot_roc_curve_with_auc(model,generator)\n",
    "    plot_misclassifications(model,generator)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
